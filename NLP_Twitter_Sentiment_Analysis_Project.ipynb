{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "name": "NLP - Twitter Sentiment Analysis Project.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/krGaurav310/AllColabProjects/blob/master/NLP_Twitter_Sentiment_Analysis_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "V9LeY6M2jTkk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries \n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "import warnings \n",
        "warnings.filterwarnings(\"ignore\", category=DeprecationWarning)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true,
        "id": "uG-neIy8jTkq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Files available in our data sets\n",
        "import os\n",
        "print(os.listdir(\"../input\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3c34552b15697f4d2129fb5fedfab72e48a73af2",
        "id": "P5vYwgD8jTku",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Loading Data\n",
        "train_df = pd.read_csv(\"../input/train_tweets.csv\")\n",
        "test_df = pd.read_csv(\"../input/test_tweets.csv\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0f83dbdee5c4006953459d805086b74c87c447ec",
        "id": "oQcIhQqfjTky",
        "colab_type": "text"
      },
      "source": [
        "Training Data Set - has 3 columns ID, Label & Tweet. Tweet columns has tweets writen by users & Label columns contains binary values 1 & 0. Where 1 represent tweet is racist/sexist and 0 represent tweet is not racist/sexist"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5285d36346dc48509b50bfb837ce560336ce448e",
        "scrolled": false,
        "id": "MFTOT-RajTkz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Training Data Set\n",
        "train_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8e7937057c2195e9db876651b4f7afcc0a2609f7",
        "id": "qq527gD2jTk4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Testing Data Set\n",
        "test_df.head()\n",
        "print('Testing data set has no Label column')\n",
        "print(test_df.head(10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bce5eb7ed68a53f60f81c79bd5f22cacf232dd59",
        "id": "OHZMTg4vjTk7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Training Data Set Information\n",
        "print(\"Training Data Set Info - Total Rows | Total Columns | Total Null Values\")\n",
        "print(train_df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d19710e504e6c7b153cc8260374d24f92733ae34",
        "id": "f5l8aZuIjTlC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Testing Data Set Information\n",
        "print(\"Test Data Set Info - Total Rows | Total Columns | Total Null Values\")\n",
        "print(test_df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "b4893bb96086a19bb9f406309bcaa29573fcfc65",
        "id": "_lThfcnhjTlH",
        "colab_type": "text"
      },
      "source": [
        "We can see in above tweet column in both data sets Training & Testing tweets are unstructured, for better analysis we first need to structure the tweets, remove the unwanted words, replace the misspelled words with the correct ones, replace the abriviation with full words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "b1a0a5ba7e0438e39fdb4361ac30e806a522db8f",
        "id": "MGm1LwrJjTlI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Merging both the data sets as tweets in both the data set is unstructured\n",
        "combine_df = train_df.append(test_df, ignore_index = True, sort = False)\n",
        "combine_df.head()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "2d080f2e485fe8fa8550cab631e7b814ec705eb4",
        "id": "mH_5mwaPjTlL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine (Merged) Data Set Information\n",
        "print(\"Combine Data Set Info - Total Rows | Total Columns | Total Null Values\")\n",
        "print(combine_df.info())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "911f61b5900d82b655ab80c5bd3ed2b9daec412e",
        "id": "FGM4FKEwjTlP",
        "colab_type": "text"
      },
      "source": [
        "We can see above, ID & Tweet column has 49159 has values where as Label column has 31962 values."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "a0a9430db2f603b9e0ed5f05e5886665dd14a754",
        "id": "9DSeWaZLjTlP",
        "colab_type": "text"
      },
      "source": [
        "## Data processing & cleaning\n",
        "* Step A : Converting html entities\n",
        "* Step B : Removing \"@user\" from all the tweets\n",
        "* Step C : Changing all the tweets into lowercase \n",
        "* Step D : Apostrophe Lookup\n",
        "* Step E : Short Word Lookup\n",
        "* Step F : Emoticon Lookup\n",
        "* Step H : Replacing Special Characters with space\n",
        "* Step I : Replacing Numbers (integers) with space\n",
        "* Step J : Removing words whom length is 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "e9d22c5851edd48a40a7b9ae1625b412fd518517",
        "id": "VN-90WsVjTlQ",
        "colab_type": "text"
      },
      "source": [
        "### Step A : Converting html entities"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "87c76d2680bd293a25d824990045c0f2cee2df18",
        "id": "9WOAYMbNjTlR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"\"\"Step A : Converting html entities i.e. (&lt; &gt; &amp;)\n",
        "( \"&lt;\" is converted to “<” and \"&amp;\" is converted to “&”)\"\"\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "177572ad8fa3630dd423ae068b53aeacec38e923",
        "id": "ngtbpYdjjTlU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing HTMLParser\n",
        "from html.parser import HTMLParser\n",
        "html_parser = HTMLParser()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "01c3c8552d1834961ed2a5be93e6acf4e095fc30",
        "id": "NRNOjLcwjTlY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Created a new columns i.e. clean_tweet contains the same tweets but cleaned version\n",
        "combine_df['clean_tweet'] = combine_df['tweet'].apply(lambda x: html_parser.unescape(x))\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "070f70352d24ab9383a3ed45cee69fb0779f9386",
        "id": "ANm67Zi5jTlb",
        "colab_type": "text"
      },
      "source": [
        "### Step B : Removing \"@user\" from all the tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "80542c3062412d5d0326fed34eed8c5afb4584ae",
        "id": "oZEyXDfejTlc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def remove_pattern(input_txt, pattern):\n",
        "    r = re.findall(pattern, input_txt)\n",
        "    for i in r:\n",
        "        input_txt = re.sub(i, '', input_txt)\n",
        "    return input_txt"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ab773581e02b71ca7273ce43eaa16fcc6753d8d7",
        "id": "U1Rz1nJyjTlf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# remove twitter handles (@user)\n",
        "combine_df['clean_tweet'] = np.vectorize(remove_pattern)(combine_df['clean_tweet'], \"@[\\w]*\")\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "4f982bc41f8136775e65abd1ca59a76bcd23198b",
        "id": "WYqe27LXjTli",
        "colab_type": "text"
      },
      "source": [
        "### Step C : Changing all the tweets into lowercase "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cd7eec4a75c1d9e44bd701927774c0aafe61b3e5",
        "id": "096Xhfi6jTlj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: x.lower())\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "9c2a5543c0ea9f020890d4aadcc732a8f30c99e2",
        "id": "A7x1ZHtcjTlm",
        "colab_type": "text"
      },
      "source": [
        "### Step D : Apostrophe Lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c8c8d24c9b6adfdef692b066dc5282a0c2e8784e",
        "id": "GUypb--DjTln",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Apostrophe Dictionary\n",
        "apostrophe_dict = {\n",
        "\"ain't\": \"am not / are not\",\n",
        "\"aren't\": \"are not / am not\",\n",
        "\"can't\": \"cannot\",\n",
        "\"can't've\": \"cannot have\",\n",
        "\"'cause\": \"because\",\n",
        "\"could've\": \"could have\",\n",
        "\"couldn't\": \"could not\",\n",
        "\"couldn't've\": \"could not have\",\n",
        "\"didn't\": \"did not\",\n",
        "\"doesn't\": \"does not\",\n",
        "\"don't\": \"do not\",\n",
        "\"hadn't\": \"had not\",\n",
        "\"hadn't've\": \"had not have\",\n",
        "\"hasn't\": \"has not\",\n",
        "\"haven't\": \"have not\",\n",
        "\"he'd\": \"he had / he would\",\n",
        "\"he'd've\": \"he would have\",\n",
        "\"he'll\": \"he shall / he will\",\n",
        "\"he'll've\": \"he shall have / he will have\",\n",
        "\"he's\": \"he has / he is\",\n",
        "\"how'd\": \"how did\",\n",
        "\"how'd'y\": \"how do you\",\n",
        "\"how'll\": \"how will\",\n",
        "\"how's\": \"how has / how is\",\n",
        "\"i'd\": \"I had / I would\",\n",
        "\"i'd've\": \"I would have\",\n",
        "\"i'll\": \"I shall / I will\",\n",
        "\"i'll've\": \"I shall have / I will have\",\n",
        "\"i'm\": \"I am\",\n",
        "\"i've\": \"I have\",\n",
        "\"isn't\": \"is not\",\n",
        "\"it'd\": \"it had / it would\",\n",
        "\"it'd've\": \"it would have\",\n",
        "\"it'll\": \"it shall / it will\",\n",
        "\"it'll've\": \"it shall have / it will have\",\n",
        "\"it's\": \"it has / it is\",\n",
        "\"let's\": \"let us\",\n",
        "\"ma'am\": \"madam\",\n",
        "\"mayn't\": \"may not\",\n",
        "\"might've\": \"might have\",\n",
        "\"mightn't\": \"might not\",\n",
        "\"mightn't've\": \"might not have\",\n",
        "\"must've\": \"must have\",\n",
        "\"mustn't\": \"must not\",\n",
        "\"mustn't've\": \"must not have\",\n",
        "\"needn't\": \"need not\",\n",
        "\"needn't've\": \"need not have\",\n",
        "\"o'clock\": \"of the clock\",\n",
        "\"oughtn't\": \"ought not\",\n",
        "\"oughtn't've\": \"ought not have\",\n",
        "\"shan't\": \"shall not\",\n",
        "\"sha'n't\": \"shall not\",\n",
        "\"shan't've\": \"shall not have\",\n",
        "\"she'd\": \"she had / she would\",\n",
        "\"she'd've\": \"she would have\",\n",
        "\"she'll\": \"she shall / she will\",\n",
        "\"she'll've\": \"she shall have / she will have\",\n",
        "\"she's\": \"she has / she is\",\n",
        "\"should've\": \"should have\",\n",
        "\"shouldn't\": \"should not\",\n",
        "\"shouldn't've\": \"should not have\",\n",
        "\"so've\": \"so have\",\n",
        "\"so's\": \"so as / so is\",\n",
        "\"that'd\": \"that would / that had\",\n",
        "\"that'd've\": \"that would have\",\n",
        "\"that's\": \"that has / that is\",\n",
        "\"there'd\": \"there had / there would\",\n",
        "\"there'd've\": \"there would have\",\n",
        "\"there's\": \"there has / there is\",\n",
        "\"they'd\": \"they had / they would\",\n",
        "\"they'd've\": \"they would have\",\n",
        "\"they'll\": \"they shall / they will\",\n",
        "\"they'll've\": \"they shall have / they will have\",\n",
        "\"they're\": \"they are\",\n",
        "\"they've\": \"they have\",\n",
        "\"to've\": \"to have\",\n",
        "\"wasn't\": \"was not\",\n",
        "\"we'd\": \"we had / we would\",\n",
        "\"we'd've\": \"we would have\",\n",
        "\"we'll\": \"we will\",\n",
        "\"we'll've\": \"we will have\",\n",
        "\"we're\": \"we are\",\n",
        "\"we've\": \"we have\",\n",
        "\"weren't\": \"were not\",\n",
        "\"what'll\": \"what shall / what will\",\n",
        "\"what'll've\": \"what shall have / what will have\",\n",
        "\"what're\": \"what are\",\n",
        "\"what's\": \"what has / what is\",\n",
        "\"what've\": \"what have\",\n",
        "\"when's\": \"when has / when is\",\n",
        "\"when've\": \"when have\",\n",
        "\"where'd\": \"where did\",\n",
        "\"where's\": \"where has / where is\",\n",
        "\"where've\": \"where have\",\n",
        "\"who'll\": \"who shall / who will\",\n",
        "\"who'll've\": \"who shall have / who will have\",\n",
        "\"who's\": \"who has / who is\",\n",
        "\"who've\": \"who have\",\n",
        "\"why's\": \"why has / why is\",\n",
        "\"why've\": \"why have\",\n",
        "\"will've\": \"will have\",\n",
        "\"won't\": \"will not\",\n",
        "\"won't've\": \"will not have\",\n",
        "\"would've\": \"would have\",\n",
        "\"wouldn't\": \"would not\",\n",
        "\"wouldn't've\": \"would not have\",\n",
        "\"y'all\": \"you all\",\n",
        "\"y'all'd\": \"you all would\",\n",
        "\"y'all'd've\": \"you all would have\",\n",
        "\"y'all're\": \"you all are\",\n",
        "\"y'all've\": \"you all have\",\n",
        "\"you'd\": \"you had / you would\",\n",
        "\"you'd've\": \"you would have\",\n",
        "\"you'll\": \"you shall / you will\",\n",
        "\"you'll've\": \"you shall have / you will have\",\n",
        "\"you're\": \"you are\",\n",
        "\"you've\": \"you have\"\n",
        "}\n",
        "apostrophe_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d95d468f06e1c6ca114238a3995d6955e024180c",
        "id": "kmMujIOsjTlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def lookup_dict(text, dictionary):\n",
        "    for word in text.split():\n",
        "        if word.lower() in dictionary:\n",
        "            if word.lower() in text.split():\n",
        "                text = text.replace(word, dictionary[word.lower()])\n",
        "    return text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "c60ac4236fddda38b0bb733741a7c1d1384ac25f",
        "id": "aWOdxPijjTlt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: lookup_dict(x,apostrophe_dict))\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ea301aa87ebcc863b13874b6cd2b2be716ce287f",
        "id": "7LY4dKPhjTlw",
        "colab_type": "text"
      },
      "source": [
        "### Step E : Short Word Lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7649b027921636733108745b19401988422de1ef",
        "id": "NmKKj_7MjTlx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "short_word_dict = {\n",
        "\"121\": \"one to one\",\n",
        "\"a/s/l\": \"age, sex, location\",\n",
        "\"adn\": \"any day now\",\n",
        "\"afaik\": \"as far as I know\",\n",
        "\"afk\": \"away from keyboard\",\n",
        "\"aight\": \"alright\",\n",
        "\"alol\": \"actually laughing out loud\",\n",
        "\"b4\": \"before\",\n",
        "\"b4n\": \"bye for now\",\n",
        "\"bak\": \"back at the keyboard\",\n",
        "\"bf\": \"boyfriend\",\n",
        "\"bff\": \"best friends forever\",\n",
        "\"bfn\": \"bye for now\",\n",
        "\"bg\": \"big grin\",\n",
        "\"bta\": \"but then again\",\n",
        "\"btw\": \"by the way\",\n",
        "\"cid\": \"crying in disgrace\",\n",
        "\"cnp\": \"continued in my next post\",\n",
        "\"cp\": \"chat post\",\n",
        "\"cu\": \"see you\",\n",
        "\"cul\": \"see you later\",\n",
        "\"cul8r\": \"see you later\",\n",
        "\"cya\": \"bye\",\n",
        "\"cyo\": \"see you online\",\n",
        "\"dbau\": \"doing business as usual\",\n",
        "\"fud\": \"fear, uncertainty, and doubt\",\n",
        "\"fwiw\": \"for what it's worth\",\n",
        "\"fyi\": \"for your information\",\n",
        "\"g\": \"grin\",\n",
        "\"g2g\": \"got to go\",\n",
        "\"ga\": \"go ahead\",\n",
        "\"gal\": \"get a life\",\n",
        "\"gf\": \"girlfriend\",\n",
        "\"gfn\": \"gone for now\",\n",
        "\"gmbo\": \"giggling my butt off\",\n",
        "\"gmta\": \"great minds think alike\",\n",
        "\"h8\": \"hate\",\n",
        "\"hagn\": \"have a good night\",\n",
        "\"hdop\": \"help delete online predators\",\n",
        "\"hhis\": \"hanging head in shame\",\n",
        "\"iac\": \"in any case\",\n",
        "\"ianal\": \"I am not a lawyer\",\n",
        "\"ic\": \"I see\",\n",
        "\"idk\": \"I don't know\",\n",
        "\"imao\": \"in my arrogant opinion\",\n",
        "\"imnsho\": \"in my not so humble opinion\",\n",
        "\"imo\": \"in my opinion\",\n",
        "\"iow\": \"in other words\",\n",
        "\"ipn\": \"I’m posting naked\",\n",
        "\"irl\": \"in real life\",\n",
        "\"jk\": \"just kidding\",\n",
        "\"l8r\": \"later\",\n",
        "\"ld\": \"later, dude\",\n",
        "\"ldr\": \"long distance relationship\",\n",
        "\"llta\": \"lots and lots of thunderous applause\",\n",
        "\"lmao\": \"laugh my ass off\",\n",
        "\"lmirl\": \"let's meet in real life\",\n",
        "\"lol\": \"laugh out loud\",\n",
        "\"ltr\": \"longterm relationship\",\n",
        "\"lulab\": \"love you like a brother\",\n",
        "\"lulas\": \"love you like a sister\",\n",
        "\"luv\": \"love\",\n",
        "\"m/f\": \"male or female\",\n",
        "\"m8\": \"mate\",\n",
        "\"milf\": \"mother I would like to fuck\",\n",
        "\"oll\": \"online love\",\n",
        "\"omg\": \"oh my god\",\n",
        "\"otoh\": \"on the other hand\",\n",
        "\"pir\": \"parent in room\",\n",
        "\"ppl\": \"people\",\n",
        "\"r\": \"are\",\n",
        "\"rofl\": \"roll on the floor laughing\",\n",
        "\"rpg\": \"role playing games\",\n",
        "\"ru\": \"are you\",\n",
        "\"shid\": \"slaps head in disgust\",\n",
        "\"somy\": \"sick of me yet\",\n",
        "\"sot\": \"short of time\",\n",
        "\"thanx\": \"thanks\",\n",
        "\"thx\": \"thanks\",\n",
        "\"ttyl\": \"talk to you later\",\n",
        "\"u\": \"you\",\n",
        "\"ur\": \"you are\",\n",
        "\"uw\": \"you’re welcome\",\n",
        "\"wb\": \"welcome back\",\n",
        "\"wfm\": \"works for me\",\n",
        "\"wibni\": \"wouldn't it be nice if\",\n",
        "\"wtf\": \"what the fuck\",\n",
        "\"wtg\": \"way to go\",\n",
        "\"wtgp\": \"want to go private\",\n",
        "\"ym\": \"young man\",\n",
        "\"gr8\": \"great\"\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d52cbb34b5c86a3242c858f294a7263d6e8c2316",
        "id": "CbLRvM8WjTl0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: lookup_dict(x,short_word_dict))\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "0f0e0be3d34256a886204fb849a5e35b7729bea3",
        "id": "7TYvYedYjTl4",
        "colab_type": "text"
      },
      "source": [
        "### Step F : Emoticon Lookup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0d73a37bf9701b53056b912d77c6ad79176aea7e",
        "id": "4rhfvUZzjTl4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "emoticon_dict = {\n",
        "\":)\": \"happy\",\n",
        "\":‑)\": \"happy\",\n",
        "\":-]\": \"happy\",\n",
        "\":-3\": \"happy\",\n",
        "\":->\": \"happy\",\n",
        "\"8-)\": \"happy\",\n",
        "\":-}\": \"happy\",\n",
        "\":o)\": \"happy\",\n",
        "\":c)\": \"happy\",\n",
        "\":^)\": \"happy\",\n",
        "\"=]\": \"happy\",\n",
        "\"=)\": \"happy\",\n",
        "\"<3\": \"happy\",\n",
        "\":-(\": \"sad\",\n",
        "\":(\": \"sad\",\n",
        "\":c\": \"sad\",\n",
        "\":<\": \"sad\",\n",
        "\":[\": \"sad\",\n",
        "\">:[\": \"sad\",\n",
        "\":{\": \"sad\",\n",
        "\">:(\": \"sad\",\n",
        "\":-c\": \"sad\",\n",
        "\":-< \": \"sad\",\n",
        "\":-[\": \"sad\",\n",
        "\":-||\": \"sad\"\n",
        "}\n",
        "emoticon_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "51cb5a05ec611e27e52ada7d8b593e1a20197aaf",
        "id": "NH5290XljTl7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: lookup_dict(x,emoticon_dict))\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "b7372f77fd0f17a8678c32da1743b408f3094745",
        "id": "P8OfifqtjTl_",
        "colab_type": "text"
      },
      "source": [
        "### Step G : ReplacingPunctuations with space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bc3e057dc4519a482f9045af600b129ddd4ce5da",
        "id": "v_NLAp8ajTmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: re.sub(r'[^\\w\\s]',' ',x))\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "633d8f73c15c4f1e6031f1e2d71ba06a4415299a",
        "id": "DOhE0TwGjTmI",
        "colab_type": "text"
      },
      "source": [
        "### Step H : Replacing Special Characters with space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "cd79a40d59cd63ca9f8898e081906fd86d1d73e2",
        "id": "9PD3pbAejTmJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z0-9]',' ',x))\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "0ad501d01f35a1c61749eebb0c1fe05db96e545d",
        "id": "Y5ENysuWjTmM",
        "colab_type": "text"
      },
      "source": [
        "### Step I : Replacing Numbers (integers) with space"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "dfe9f109d9f6d033148e62cb83af4dad8a14e2b1",
        "id": "G14dj09wjTmN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: re.sub(r'[^a-zA-Z]',' ',x))\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "b1e0b7e96ea8f3d4d12dcc0847176802c41c16d3",
        "id": "GLbdkv7bjTmQ",
        "colab_type": "text"
      },
      "source": [
        "### Step J : Removing words whom length is 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "9a175a37f356dc2598fed8a68a708c4a3db3904e",
        "id": "ALNDLBtKjTmR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "combine_df['clean_tweet'] = combine_df['clean_tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>1]))\n",
        "combine_df['clean_tweet'][0:5]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "9296825e560e450b8b96210211a47f296ca8e451",
        "id": "OiV7g6lyjTmU",
        "colab_type": "text"
      },
      "source": [
        "### Step K : Spelling Correction - With TextBlob Library\n",
        "* ### See how textblob works, short introduction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "0adcc25a7b91e10cabb025162fff03f6ce65611f",
        "id": "gXzgfEaqjTmV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from textblob import TextBlob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7380879e7c704205bb601c9f9bdedc213d735b83",
        "id": "vlhAgPyLjTmY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Spelling correction is a cool feature which TextBlob offers, we can be accessed using the correct function as shown below.\n",
        "blob = TextBlob(\"Why are you stting on this bech??\") # Scentence with two errors\n",
        "print(blob.correct()) # Correct function give us the best possible word simmilar to \"gret\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "42aae2c8923eaaf9c51cf969c58c60db752b7cee",
        "id": "mMhdqWAvjTmc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# we can see all the similar matches our first error along with the probability score.\n",
        "blob.words[3].spellcheck()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c0be5bbe54361e76ac55c216f8dee511d3c6726d",
        "id": "hNatlyBZjTmf",
        "colab_type": "text"
      },
      "source": [
        "### Applying TextBlob on our data set - Spelling correction"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "3ceb7c6fdab1e50b2e1d33f53e67b5345b3e122a",
        "id": "OI20MkBAjTmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Not cleaning the just showing the spelling check as its take lot of time to process all these tweets\n",
        "## Shown sample how its must done\n",
        "text = combine_df['clean_tweet'][0:10].apply(lambda x: str(TextBlob(x).correct()))\n",
        "text"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5f61e7e721f920b2d81445689c8e4c492095b91d",
        "id": "xGcz4M8-jTmh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing stop words from NLTK coupus and word tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "861383a5269931c29e6a1b0efa3fc85f69d76571",
        "id": "ImiifPyLjTmk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating token for the clean tweets\n",
        "combine_df['tweet_token'] = combine_df['clean_tweet'].apply(lambda x: word_tokenize(x))\n",
        "\n",
        "## Fully formated tweets & there tokens\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "45743abd92fd353287751fa3cb3990728b680cbc",
        "id": "pAeE7onIjTmm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing stop words from NLTK corpus for english language\n",
        "stop_words = set(stopwords.words('english'))\n",
        "stop_words"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bdbbaa375fc6be7787029b585c13bdbc38de9ad0",
        "id": "w-T0MvKujTmp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Created new columns of tokens - where stop words are being removed\n",
        "combine_df['tweet_token_filtered'] = combine_df['tweet_token'].apply(lambda x: [word for word in x if not word in stop_words])\n",
        "\n",
        "## Tokens columns with stop words and without stop words\n",
        "combine_df[['tweet_token', 'tweet_token_filtered']].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "ec0581e36b4e660ed22fc5b97ad821efe90fba5f",
        "id": "gwfDe8JZjTmu",
        "colab_type": "text"
      },
      "source": [
        "## We will create 2 new columns\n",
        "* One For Stemming\n",
        "* Second For Lemmatization\n",
        "\n",
        "The difference between stemming and lemmatization is, lemmatization considers the context and converts the word to its meaningful base form, whereas stemming just removes the last few characters, often leading to incorrect meanings and spelling errors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "817c6a61088ff8c007c77501abba1ece60e6a04a",
        "id": "cMU1KzjYjTmu",
        "colab_type": "text"
      },
      "source": [
        "### Stemming - Stemming refers to the removal of suffices, like “ing”, “ly”, “s”, etc. by a simple rule-based approach."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "015e8fd0be588c6d9a096a8ee3ee9c61da11ea0e",
        "id": "yOSmDX9pjTmv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing library for stemming\n",
        "from nltk.stem import PorterStemmer\n",
        "stemming = PorterStemmer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "7bbb86932936fc74c0715743ad4a2df2ff237f3c",
        "id": "wWR_h8X8jTmx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Created one more columns tweet_stemmed it shows tweets' stemmed version\n",
        "combine_df['tweet_stemmed'] = combine_df['tweet_token_filtered'].apply(lambda x: ' '.join([stemming.stem(i) for i in x]))\n",
        "combine_df['tweet_stemmed'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "f7ca4ae22d3190c714a6710e0f9329a27b74b7eb",
        "id": "GRk5k5-MjTmz",
        "colab_type": "text"
      },
      "source": [
        "### Lemmatization - Lemmatization is the process of converting a word to its base form."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "045aa82aa4843622dfaa011fe21ca3bfc1678d66",
        "id": "OvUqC-JYjTm0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing library for lemmatizing\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lemmatizing = WordNetLemmatizer()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fa0e2bd81229d3bcaf9ebe2b759606899b1e46fe",
        "id": "SjO7jK72jTm4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Created one more columns tweet_lemmatized it shows tweets' lemmatized version\n",
        "combine_df['tweet_lemmatized'] = combine_df['tweet_token_filtered'].apply(lambda x: ' '.join([lemmatizing.lemmatize(i) for i in x]))\n",
        "combine_df['tweet_lemmatized'].head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8a52bc2061d51f8532d4ca1fe9210286f99ee529",
        "id": "XQ_H2JlJjTnB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Our final dataframe - Fully formatted, Processed, Noise less, Cleaned, ready to analyse\n",
        "## for further analysis we consider 2 columns i.e. \"tweet_stemmed\" & \"tweet_lematized\"\n",
        "### We are using 2 columns to see which of them give us better score.\n",
        "combine_df.head(10)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "f7d505931b3be29d0f2e56e1301e0919b4b727a7",
        "id": "bOT8_P2rjTnE",
        "colab_type": "text"
      },
      "source": [
        "## Now When Our Data Is Cleaned & Ready We Start Our Text Analysis\n",
        "### We will do our analysis on two columns i.e. \"tweet_stemmed\" & \"tweet_lematized\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "f6dc375938cbcc48dd98333df66b4992c188153c",
        "id": "D4ygUJGgjTnF",
        "colab_type": "text"
      },
      "source": [
        "### A - Will see the most commonly used words for both the columns i.e. \"tweet_stemmed\" & \"tweet_lematized\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "549f511140c37bdaedad8ca5ec9d1610b1663f8e",
        "id": "z4vdgafFjTnF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#visualizing all the words in column \"tweet_stemmed\" in our data using the wordcloud plot.\n",
        "all_words = ' '.join([text for text in combine_df['tweet_stemmed']])\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Most Common words in column Tweet Stemmed\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "8be8dbb731bbf61bd1c4662311b545ba6d1a79b8",
        "id": "3bIot-93jTnI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing all the words in column \"tweet_lemmatized\" in our data using the wordcloud plot.\n",
        "all_words = ' '.join([text for text in combine_df['tweet_lemmatized']])\n",
        "from wordcloud import WordCloud\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n",
        "\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Most Common words in column Tweet Lemmatized\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "958635bd7cbd0d075d2836909b02a24c59fea2dc",
        "id": "fmZYN-aOjTnO",
        "colab_type": "text"
      },
      "source": [
        "### B) Most common words in non racist/sexist tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "a62781dff0051a8983a1fff31f7a460834e98134",
        "id": "xIo1cGLHjTnQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing all the normal or non racist/sexist words in column \"tweet_stemmed\" in our data using the wordcloud plot.\n",
        "normal_words =' '.join([text for text in combine_df['tweet_stemmed'][combine_df['label'] == 0]])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Most non racist/sexist words in column Tweet Stemmed\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "ef5d59ef45f50c2a928f1a7f0d0db600eef98091",
        "id": "JRgnUHX9jTnU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing all the normal or non racist/sexist words in column \"tweet_lemmatized\" in our data using the wordcloud plot.\n",
        "normal_words =' '.join([text for text in combine_df['tweet_lemmatized'][combine_df['label'] == 0]])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Most non racist/sexist words in column Tweet Lemmatized\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "a02dfb3d7dffc9e359b7ed06074f505dd8e2a327",
        "id": "hZgr7eKhjTnZ",
        "colab_type": "text"
      },
      "source": [
        "## In above plots we can see most of the words are positive or neutral. With happy and love being the most frequent ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "72cc53a676d05b59b9aaf2be13efa1e922ed499b",
        "id": "z1qeWqXSjTnZ",
        "colab_type": "text"
      },
      "source": [
        "### C) Most common words in racist/sexist tweets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "4df9a06e4b023ebd657bad97ea9cdf451a1cc84c",
        "id": "Nfd9WawbjTna",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing all the negative or racist/sexist words in column \"tweet_stemmed\" in our data using the wordcloud plot.\n",
        "negative_words =' '.join([text for text in combine_df['tweet_stemmed'][combine_df['label'] == 1]])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Most racist/sexist words in column Tweet Stemmed\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "070a4cef8e8caef4ca302270708365297730fa67",
        "id": "5zsaF1IxjTnc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Visualizing all the negative or racist/sexist words in column \"tweet_lemmatized\" in our data using the wordcloud plot.\n",
        "negative_words =' '.join([text for text in combine_df['tweet_lemmatized'][combine_df['label'] == 1]])\n",
        "\n",
        "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(negative_words)\n",
        "plt.figure(figsize=(10, 7))\n",
        "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
        "plt.axis('off')\n",
        "plt.title(\"Most racist/sexist words in column Tweet Lemmatized\")\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "adc7074aa14edbb4e527f845acdc620ef24eacfc",
        "id": "EZm47wVBjTnf",
        "colab_type": "text"
      },
      "source": [
        "## In above plots we can see most of the words are negative or racist/sexist. With racist, hate and black being the most frequent ones."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d9b0bc92591dd62b237164b62a182a7a211465e8",
        "id": "WyzTQ5BvjTnf",
        "colab_type": "text"
      },
      "source": [
        "# Extracting Features from Cleaned Tweets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "d6953dd453a2e1dcdbf4e53774b0a789665ed8ef",
        "id": "HFcsJ0EYjTnf",
        "colab_type": "text"
      },
      "source": [
        "## A - Bag-of-Words Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "61f322cffd2d60134a1a33df0fd257b8773d6676",
        "id": "uawe5u20jTng",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing library\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "bow_vectorizer = CountVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "bow_vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "560b6b55a7bca69727ea8ec2c7197c10f9b43e35",
        "id": "2iv-nJYdjTnl",
        "colab_type": "text"
      },
      "source": [
        "## A.1 Bag-Of-Words feature matrix - For columns \"combine_df['tweet_stemmed']\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "179ae876bfa4943c6012bb3767a1edf80d7ffc7b",
        "id": "isGaCo5vjTnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bag-of-words feature matrix - For columns \"combine_df['tweet_stemmed']\"\n",
        "bow_stem = bow_vectorizer.fit_transform(combine_df['tweet_stemmed'])\n",
        "bow_stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "c1204a1fbec5613ffe54d8dbde9f2cc18794b9f7",
        "id": "shY2jtsLjTnq",
        "colab_type": "text"
      },
      "source": [
        "## A.2 Bag-Of-Words feature matrix - For column - combine_df['tweet_lemmatized']"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d857a34457edb9d42fb5ed33ef33713ec8c011ea",
        "id": "4UtI5YKdjTnq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# bag-of-words feature matrix - For column - combine_df['tweet_lemmatized']\n",
        "bow_lemm = bow_vectorizer.fit_transform(combine_df['tweet_lemmatized'])\n",
        "bow_lemm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "7b0fcd11dd852db6e0c30bc852233102f2cdab1b",
        "id": "fqTtupWejTnu",
        "colab_type": "text"
      },
      "source": [
        "## B - TF-IDF Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5326dd3054b90c58480bf3c0d8e8f5469685a18d",
        "id": "zYnirkARjTnv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing library\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "tfidf_vectorizer = TfidfVectorizer(max_df=0.90, min_df=2, max_features=1000, stop_words='english')\n",
        "tfidf_vectorizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "5bf8d19cff1c4c381d3963090362513181708f2c",
        "id": "jjCoQ78SjTn0",
        "colab_type": "text"
      },
      "source": [
        "## B.1 TF-IDF feature matrix - For columns \"combine_df['tweet_stemmed']\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "bba8ac94b1cb654e0851bd991aabdd427987c54e",
        "id": "fn0wOJ60jTn1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF-IDF feature matrix - For columns \"combine_df['tweet_stemmed']\"\n",
        "tfidf_stem = tfidf_vectorizer.fit_transform(combine_df['tweet_stemmed'])\n",
        "tfidf_stem"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "3c325c4f8f08b06cf51f7317f3538dc78fd0e21b",
        "id": "tiY2Ibr1jTn4",
        "colab_type": "text"
      },
      "source": [
        "## B.2 TF-IDF feature matrix - For columns \"combine_df['tweet_lemmatized']\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "fa00d1e2dd0f17c7fc6b20a571040868576afb0f",
        "scrolled": true,
        "id": "JZE1JklJjTn8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# TF-IDF feature matrix - For columns \"combine_df['tweet_lemmatized']\"\n",
        "tfidf_lemm = tfidf_vectorizer.fit_transform(combine_df['tweet_lemmatized'])\n",
        "tfidf_lemm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "3a09234051a999fbdf538b299a5270ee706acc70",
        "id": "slaZX49ZjToA",
        "colab_type": "text"
      },
      "source": [
        "# Logistic Regression Model Building: Twitter Sentiment Analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "abd0b8d9de69a6ce9587f82179deb7db77f10318",
        "id": "IPkZHniLjToB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Importing Libraries\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "88d2065edacbbc8123fc57097e3eba5d8ad9cab0",
        "id": "WcM6eqCBjToE",
        "colab_type": "text"
      },
      "source": [
        "## A Building model using Bag-of-Words features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "c8ab0e0c6b131fc7d38e4dc8eff35f1ef05d878d",
        "id": "ka50ag3GjToF",
        "colab_type": "text"
      },
      "source": [
        "## A.1 For columns \"combine_df['tweet_stemmed']\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "5757e1e6d333256642007fb8ae91a242c0e0b236",
        "id": "F-slhrRjjToF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A.1 For columns \"combine_df['tweet_stemmed']\"\n",
        "train_bow = bow_stem[:31962,:]\n",
        "test_bow = bow_stem[31962:,:]\n",
        "\n",
        "# splitting data into training and validation set\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train_df['label'], random_state=42, test_size=0.3)\n",
        "\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_bow, ytrain) # training the model\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
        "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "A1 = f1_score(yvalid, prediction_int) # calculating f1 score\n",
        "print(A1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "9fbedbe155e01cca12a4dac5dc62f8b501058e42",
        "id": "qUbr5GkHjToQ",
        "colab_type": "text"
      },
      "source": [
        "## A.2 For columns \"combine_df['tweet_lemmatized']\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "653c702b28e2a14b7a07ab878f3e87bd3bdb59be",
        "id": "41fl-smjjToR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# A.2 For columns \"combine_df['tweet_lemmatized']\"\n",
        "train_bow = bow_lemm[:31962,:]\n",
        "test_bow = bow_lemm[31962:,:]\n",
        "\n",
        "# splitting data into training and validation set\n",
        "xtrain_bow, xvalid_bow, ytrain, yvalid = train_test_split(train_bow, train_df['label'], random_state=42, test_size=0.3)\n",
        "\n",
        "lreg = LogisticRegression()\n",
        "lreg.fit(xtrain_bow, ytrain) # training the model\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_bow) # predicting on the validation set\n",
        "prediction_int = prediction[:,1] >= 0.3 # if prediction is greater than or equal to 0.3 than 1 else 0\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "A2 = f1_score(yvalid, prediction_int) # calculating f1 score\n",
        "print(A2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "a69b89e018f1e083422fd3ae23107504e0de6fe5",
        "id": "66iQGk3ajToU",
        "colab_type": "text"
      },
      "source": [
        "## B Building model using TF-IDF features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "trusted": true,
        "_uuid": "4a247de39933828824245984e9e8a706de8da21d",
        "id": "7eItXdjgjToU",
        "colab_type": "text"
      },
      "source": [
        "## B.1 For columns \"combine_df['tweet_stemmed']\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "e4cc823825615c83174caa86ccc5530cfc63c2f3",
        "id": "MRO0telujToV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B.1 For columns \"combine_df['tweet_stemmed']\"\n",
        "train_tfidf = tfidf_stem[:31962,:]\n",
        "test_tfidf = tfidf_stem[31962:,:]\n",
        "\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]\n",
        "\n",
        "lreg.fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_tfidf)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "B1 = f1_score(yvalid, prediction_int) # calculating f1 score\n",
        "print(B1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "_uuid": "bb0b3a97022a15eda2d8535e8708992539f6e745",
        "id": "RAwK-FEljToY",
        "colab_type": "text"
      },
      "source": [
        "## B.2 For columns \"combine_df['tweet_lemmatized']\""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "f07bbd580c9157b39f485d5cf006fce4d4796edc",
        "id": "Os53OdCljToY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# B.2 For columns \"combine_df['tweet_lemmatized']\"\n",
        "train_tfidf = tfidf_lemm[:31962,:]\n",
        "test_tfidf = tfidf_lemm[31962:,:]\n",
        "\n",
        "xtrain_tfidf = train_tfidf[ytrain.index]\n",
        "xvalid_tfidf = train_tfidf[yvalid.index]\n",
        "\n",
        "lreg.fit(xtrain_tfidf, ytrain)\n",
        "\n",
        "prediction = lreg.predict_proba(xvalid_tfidf)\n",
        "prediction_int = prediction[:,1] >= 0.3\n",
        "prediction_int = prediction_int.astype(np.int)\n",
        "\n",
        "B2 = f1_score(yvalid, prediction_int) # calculating f1 score\n",
        "print(B2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "trusted": true,
        "_uuid": "d06bdef79f659b52913876d97071a512f10a9c91",
        "id": "GpmL_Fb3jTob",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(\"F1 - Score Chart\")\n",
        "print(\"* F1-Score - Model using Bag-of-Words features\")\n",
        "print(\"   F1-Score = \",A1,\" - For column tweets are stemmed\")\n",
        "print(\"   F1-Score = \",A2,\" - For column tweets are Lemmatized\")\n",
        "print(\"* F1-Score - Model using TF-IDF features\")\n",
        "print(\"   F1-Score = \",B1,\" - For column tweets are stemmed\")\n",
        "print(\"   F1-Score = \",B2,\" - For column tweets are Lemmatized\")\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}